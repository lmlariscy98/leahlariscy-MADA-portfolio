---
title: "Model Evaluation"
author: "Leah Lariscy"
editor: visual
format:
  html:
    toc: true
    toc-depth: 3
---

## Library packages

```{r, message=FALSE}
library(tidymodels)
library(tidyverse)
library(here)
library(dotwhisker)
```

## Load Data

```{r}
symptoms_fit <- readRDS(here("fluanalysis/data/processed_data/symptoms_clean.RDS"))
colnames(symptoms_fit) #quick look at data
```

## Data Split

```{r}
data_split <- initial_split(symptoms_fit, prop = 3/4) # 75% of data goes into training set

train_data <- training(data_split)
test_data <- testing(data_split)
```

## Model 1 fitting: all variables to predict nausea

### Define the model: logistic regression

```{r}
log_mod <- logistic_reg() %>% #model type is logistic regression 
  set_engine("glm") #engine set to generalized linear model
```

I am using a logistic regression here because the outcome of interest (Nausea Y/N) is categorical

### Create recipe

```{r}
recipe_nausea <- recipe(Nausea ~., data = train_data)
```

Nausea is the outcome and all other variables are predictors

### Create workflow: combine model definition and recipe

```{r}
nausea_log_wf <- workflow() %>% 
  add_model(log_mod) %>% #model definition 
  add_recipe(recipe_nausea) #model recipe
```

This will run a logistic regression on the flu data, predicting nausea using all other variables that we kept.

### Model fitting

```{r}
set.seed(626)
nausea_fit <- nausea_log_wf %>% 
  fit(data = train_data)

nausea_fit %>% extract_fit_parsnip() %>% 
  tidy()
```

### Model assessment on training data: ROC curve

```{r, message=FALSE, warning=FALSE}
set.seed(626)
nausea_aug_test <- augment(nausea_fit, train_data)

nausea_aug_test %>% 
  roc_curve(truth = Nausea, .pred_Yes, event_level = "second") %>% 
  autoplot()

nausea_aug_test %>% roc_auc(truth = Nausea, .pred_Yes, 
                            event_level = "second")
```

ROC-AUC is ok

### Model assessment on testing data: ROC curve

```{r, message=FALSE, warning=FALSE}
set.seed(626)
nausea_fit_test <- nausea_log_wf %>% 
  fit(data = test_data)

nausea_fit_test %>% extract_fit_parsnip() %>% 
  tidy()
nausea_aug_test2 <- augment(nausea_fit_test, test_data)

nausea_aug_test2 %>% 
  roc_curve(truth = Nausea, .pred_Yes, event_level = "second") %>% 
  autoplot()

nausea_aug_test2 %>% roc_auc(truth = Nausea, .pred_Yes, 
                            event_level = "second")
```

The testing data out-performed the training set

## Model 2 fitting: runny nose to predict nausea

### Create new recipe and workflow

```{r}
recipe_nausea2 <- recipe(Nausea ~RunnyNose, data = train_data) #only include runny nose

nausea_log_wf2 <- workflow() %>% 
  add_model(log_mod) %>% #model definition, use the same as Model 1
  add_recipe(recipe_nausea2) #model recipe
```

### Model fitting

```{r}
set.seed(626)
nausea_fit2 <- nausea_log_wf2 %>% 
  fit(data = train_data)

nausea_fit2 %>% extract_fit_parsnip() %>% 
  tidy()
```

### Model assessment on training data

```{r, message=FALSE}
set.seed(626)
nausea_aug_test2 <- augment(nausea_fit2, train_data)

nausea_aug_test2 %>% 
  roc_curve(truth = Nausea, .pred_Yes, event_level = "second") %>% 
  autoplot()

nausea_aug_test2 %>% roc_auc(truth = Nausea, .pred_Yes, 
                            event_level = "second")
```

The ROC-AUC here is lower than Model 1

### Model assessment on testing data

```{r, message=FALSE}
set.seed(626)
nausea_fit_test3 <- nausea_log_wf2 %>% 
  fit(data = test_data)

nausea_fit_test3 %>% extract_fit_parsnip() %>% 
  tidy()
nausea_aug_test3 <- augment(nausea_fit_test3, test_data)

nausea_aug_test3 %>% 
  roc_curve(truth = Nausea, .pred_Yes, event_level = "second") %>% 
  autoplot()

nausea_aug_test3 %>% roc_auc(truth = Nausea, .pred_Yes, 
                            event_level = "second")
```

The testing data is no different. Runny nose is likely not a good predictor of nausea.

# This Section added by Nathan Greenslit

## Model 3 fitting: all variables to predict Body Temperature

### Define the model: linear regression

```{r}
lin_mod <- linear_reg() %>% #model type is linear regression 
  set_engine("lm") #engine set to  linear model
```

### Create recipe for Multivariate model

```{r}
recipe_bt_all <- recipe(BodyTemp ~., data = train_data)
```

Body Temp is the outcome and all other variables are predictors

### Create workflow: combine model definition and recipe

```{r}
bt_lin_wf_all <- workflow() %>% 
  add_model(lin_mod) %>% #model definition 
  add_recipe(recipe_bt_all) #model recipe
```

This will run a linear regression on the flu data, predicting body temperature [using all other variables]{.underline} that we kept.

### Multivariate Model Fitting

```{r}
bt_fit_all <- bt_lin_wf_all %>% 
  fit(data = train_data)

bt_fit_all %>% extract_fit_parsnip() %>% 
  tidy()
```

### Model assessment on [training]{.underline} data: RMSE and Rsq

```{r}
set.seed(626)
bt_aug_train_all <- augment(bt_fit_all, train_data) #pull in train data

### RMSE and R2 to test model performance

rmse1 <- bt_aug_train_all %>% rmse(truth = BodyTemp, .pred) #RMSE
rsq1 <- bt_aug_train_all %>% rsq(truth = BodyTemp, .pred) #RSQ
bt_metrics_all_train<- full_join(rmse1, rsq1)
bt_metrics_all_train
```

### Model assessment on [testing]{.underline} data: RMSE and Rsq

```{r}
set.seed(626)
bt_fit_test_all <- bt_lin_wf_all %>% 
  fit(data = test_data) #pull in test data

bt_fit_test_all %>% extract_fit_parsnip() %>% 
  tidy()

bt_aug_test_all <- augment(bt_fit_test_all, test_data)

### RMSE and R2 to test model performance

rmse2 <- bt_aug_test_all %>% rmse(truth = BodyTemp, .pred) #RMSE
rsq2 <- bt_aug_test_all %>% rsq(truth = BodyTemp, .pred) #RSQ
bt_metrics_all_test<- full_join(rmse2, rsq2)
bt_metrics_all_test
```

Here it looks like the test data outperformed the model data (lower rmse and higher rsq).

## Model 4 fitting: runny nose to predict Body Temperature

### Create new recipe and workflow

```{r}
recipe_bt2 <- recipe(BodyTemp ~RunnyNose, data = train_data) #only include runny nose

bt_lin_wf2 <- workflow() %>% 
  add_model(lin_mod) %>% #model definition, use the same as Model 1
  add_recipe(recipe_bt2) #model recipe
```

### Model fitting

```{r}
bt_fit2 <- bt_lin_wf2 %>% 
  fit(data = train_data)

bt_fit2 %>% extract_fit_parsnip() %>% 
  tidy()
```

### Model assessment on [training]{.underline} data

```{r}
set.seed(626)
bt_aug_train_uni <- augment(bt_fit2, train_data) #Train Data

### RMSE and R2 to test model performance

rmse3 <- bt_aug_train_uni %>% rmse(truth = BodyTemp, .pred) #RMSE
rsq3 <- bt_aug_train_uni %>% rsq(truth = BodyTemp, .pred) #RSQ
bt_metrics_uni_train<- full_join(rmse3, rsq3)
bt_metrics_uni_train
```

### Model assessment on [testing]{.underline} data

```{r}
set.seed(626)
bt_fit_test_uni <- bt_lin_wf2 %>% 
  fit(data = test_data)

bt_fit_test_uni %>% extract_fit_parsnip() %>% 
  tidy()
bt_aug_test_uni <- augment(bt_fit_test_uni, test_data) #Test Data

### RMSE and R2 to test model performance

rmse4 <- bt_aug_test_uni %>% rmse(truth = BodyTemp, .pred) #RMSE
rsq4 <- bt_aug_test_uni %>% rsq(truth = BodyTemp, .pred) #RSQ
bt_metrics_uni_test<- full_join(rmse4, rsq4)
bt_metrics_uni_test
```

Here, the training data outperformed the model data, but either way, it does not look like Runny nose is a good predictor of Body Temperature.

Ultimately, our multivariate linear regression provided better performance assessments, so there is more exploring to be done!
