---
title: "Machine Learning"
editor: visual
format:
  html:
    toc: true
    toc-depth: 3
---

## Load packages

```{r, message=FALSE}
library(here)
library(tidyverse)
library(rsample) #Data split
library(tidymodels)
library(rpart) #Model fit
library(ranger) #Model fit
library(glmnet) #Model fit
library(rpart.plot)  #viz of decision tree
library(vip) #viz of variable importance plots
library(ggpmisc) #for adding linear regression to plots
```

## Load data

```{r}
data <- readRDS(here("fluanalysis/data/processed_data/symptoms_clean.RDS"))
```

## Data splitting

```{r}
set.seed(123)

data_split <- initial_split(
  data, prop = 7/10, #70:30 Split
  strata = BodyTemp) #more balanced outcomes across train/test split
  
train <- training(data_split)
test  <- testing(data_split)
```

## Create recipes for later use

```{r}
#Training data
bt_rec_train <- recipe(BodyTemp~., data = train) %>% 
  step_dummy(all_nominal(), -all_outcomes()) %>% #pick nominal predictors
  step_ordinalscore() %>%
  step_zv(all_predictors()) 

#Testing data
bt_rec_test <- recipe(BodyTemp~., data = test) %>% 
  step_dummy(all_nominal(), -all_outcomes()) %>% #pick nominal predictors
  step_ordinalscore() %>%
  step_zv(all_predictors()) 
```

# Null Model

### 5-Fold Cross Validation

```{r}
fold_bt_train <- vfold_cv(train, v = 5, repeats = 5, strata = BodyTemp)

fold_bt_test <- vfold_cv(test, v = 5, repeats = 5, strata = BodyTemp)
```

### Create recipe

```{r}
#Train Data
rec_train <- 
  recipe(BodyTemp ~ ., data = train) %>% #predict BodyTemp with all variables
  step_dummy(all_nominal(), -all_outcomes()) %>% #pick nominal predictors
  step_ordinalscore() %>%
  step_zv(all_predictors()) 

#Test Data
rec_test <- 
  recipe(BodyTemp ~ ., data = train) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_ordinalscore() %>%
  step_zv(all_predictors()) 
```

### Define Model

```{r}
lm_mod <- linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression")
```

### Create workflows

```{r}
#training data
null_train_wf <- workflow() %>% 
  add_model(lm_mod) %>% 
  add_recipe(bt_rec_train)

#testing data
null_test_wf <- workflow() %>% 
  add_model(lm_mod) %>% 
  add_recipe(bt_rec_test)
```

### Fit models

```{r, message=FALSE}
#training data
null_train_fit <- fit_resamples(null_train_wf, resamples = fold_bt_train)

null_test_fit <- fit_resamples(null_test_wf, resamples = fold_bt_test)

```

### Compare train/test metrics

```{r}
null_train_met <- collect_metrics(null_train_fit)
null_train_met

null_test_met <- collect_metrics(null_test_fit)
null_test_met
```

Training RMSE: 1.21

Testing RMSE: 1.16

# Tree model tuning and fitting

### Specify model

```{r}
tune_spec_dtree <- decision_tree(
    cost_complexity = tune(),
    tree_depth = tune()) %>%
  set_engine("rpart") %>% 
  set_mode("regression")

tune_spec_dtree
```

### Create workflow

```{r}
dtree_wf <- workflow() %>%
  add_model(tune_spec_dtree) %>%
  add_recipe(bt_rec_train)
```

### Create grid for model tuning

```{r}
tree_grid_dtree <-
  grid_regular(
    cost_complexity(), 
    tree_depth(), 
    levels = 5)

tree_grid_dtree
```

### Use cross-validation for tuning

```{r, message=FALSE}
dtree_resample <- dtree_wf %>% 
  tune_grid(
    resamples = fold_bt_train,
    grid = tree_grid_dtree)
```

```{r}
dtree_resample %>%
  collect_metrics()
```

### Model plotting

```{r}
dtree_resample %>% collect_metrics() %>%
  mutate(tree_depth = factor(tree_depth)) %>%
  ggplot(aes(cost_complexity, mean, color = tree_depth)) +
  geom_line(linewidth = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0)
```

### Select the best model

The [`show_best()`](https://tune.tidymodels.org/reference/show_best.html) function shows us the top 5 candidate models by default. We set n=1

```{r}
dtree_resample %>%
  show_best(n=1)
```

Tree model RMSE: 1.23

Tree depth of 1 has the best performance metrics

```{r, message=FALSE, warning=FALSE}
#Selects best performing model
best_tree <- dtree_resample %>%
  select_best() #pull out best performing hyperparameters

best_tree
```

### Create final fit based on best model

#### Update workflow

```{r}
dtree_final_wf <- dtree_wf %>% 
  finalize_workflow(best_tree) #update workflow with values from select_best()

dtree_final_wf
```

#### Fit model to training data

```{r}
dtree_final_fit <- dtree_final_wf %>%
  fit(train) 
```

### Evaluate final fit

#### Calculating residuals

```{r}
dtree_residuals <- dtree_final_fit %>%
  augment(train) %>% #use augment() to make predictions
  select(c(.pred, BodyTemp)) %>%
  mutate(.resid = BodyTemp - .pred) #calculate residuals

dtree_residuals
```

#### Plot predictions vs. true value

```{r}
dtree_pred_plot <- ggplot(dtree_residuals, 
                          aes(x = BodyTemp, 
                              y = .pred)) + 
  geom_point() + 
  labs(title = "Predictions vs Actual: Decision Tree", 
       x = "Body Temperature Outcome", 
       y = "Body Temperature Prediction")
dtree_pred_plot
```

#### Plot residuals vs. predictions

```{r}
dtree_residual_plot <- ggplot(dtree_residuals, 
                              aes(y = .resid, 
                                  x = .pred)) + 
  geom_point() + 
  labs(title = "Predictions vs Residuals: Decision Tree", 
       x = "Body Temperature Prediction", 
       y = "Residuals")
plot(dtree_residual_plot) #view plot
```

## LASSO model tuning and fitting

### Specify Model

```{r}
lasso_mod <- linear_reg(penalty = tune(), mixture = 1) %>% 
  set_engine("glmnet")
```

Setting `mixture` to a value of one means that the glmnet model will potentially remove irrelevant predictors and choose a simpler model.

### Create Workflow

```{r}
lasso_wf <- workflow() %>%
  add_model(lasso_mod) %>%
  add_recipe(bt_rec_train)
```

### Creating a Tuning Grid

```{r}
lasso_grid <- tibble(penalty = 10^seq(-3, 0, length.out = 30))

```

### Use cross-validation for tuning

```{r, message=FALSE}
lasso_resample <- lasso_wf %>%
  tune_grid(resamples = fold_bt_train,
            grid = lasso_grid,
            control = control_grid(verbose = FALSE, save_pred = TRUE),
            metrics = metric_set(rmse))

lasso_resample %>%
  collect_metrics()
```

### Model plotting

```{r}
lr_plot <- lasso_resample %>% 
  collect_metrics() %>% 
  ggplot(aes(x = penalty, y = mean)) + 
  geom_point() + 
  geom_line() +
  scale_x_log10(labels = scales::label_number())

lr_plot
```

### Select the best model

```{r}
#Show best model
lasso_resample %>% show_best(n=1)

#Select best model
best_lasso <- lasso_resample %>%
  select_best()
```

Lasso model RMSE: 1.18 (slightly better than Tree)

### Create final fit based on best model

```{r}
lasso_final_wf <- lasso_wf %>% 
  finalize_workflow(best_lasso) #update workflow with best model

lasso_final_fit <- lasso_final_wf %>%
  fit(train) #fit updated workflow to training data
```

### Evaluate final fit

#### Calculating residuals

```{r}
lasso_residuals <- lasso_final_fit %>%
  augment(train) %>% #use augment() to make predictions from train data
  select(c(.pred, BodyTemp)) %>%
  mutate(.resid = BodyTemp - .pred) #calculate residuals and make new row.

lasso_residuals
```

#### Plot predictions vs. true value

```{r}
lasso_pred_plot <- ggplot(lasso_residuals, 
                          aes(x = BodyTemp, 
                              y = .pred)) + 
  geom_point() + 
  labs(title = "Predictions vs Actual: LASSO", 
       x = "Body Temperature Outcome", 
       y = "Body Temperature Prediction") +
  stat_poly_line() +
  stat_poly_eq()
lasso_pred_plot


```

RSQ is low but there is still a positive correlation there (which is good)

#### Plot residuals vs. predictions

```{r}
lasso_residual_plot <- ggplot(lasso_residuals, 
                              aes(y = .resid, 
                                  x = .pred)) + 
  geom_point() + 
  labs(title = "Predictions vs Residuals: LASSO", 
       x = "Body Temperature Prediction", 
       y = "Residuals") +
   stat_poly_line() +
  stat_poly_eq()
plot(lasso_residual_plot) #view plot
```

There should be no relationship here, which there isn't.

## Random Forest model tuning and fitting

### Detect cores for RFM computation

```{r}
cores <- parallel::detectCores()
cores
```

### Specify Model

```{r}
rf_mod <- rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
  set_engine("ranger", num.threads = cores) %>% 
  set_mode("regression")
```

### Create workflow

```{r}
rf_wf <- workflow() %>% add_model(rf_mod) %>%
  add_recipe(bt_rec_train)
```

### Create grid for model tuning

```{r}
rf_grid  <- expand.grid(mtry = c(3, 4, 5, 6),
                        min_n = c(40,50,60), 
                        trees = c(500,1000))
```

### Use cross-validation for tuning

```{r, message=FALSE}
rf_resample <- rf_wf %>% 
  tune_grid(fold_bt_train,
            grid = 25,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rmse))
```

#### Check CV metrics

```{r}
rf_resample %>%
  collect_metrics()
```

### Model performance plotting

```{r}
rf_resample %>% autoplot()
```

### Select the best model

```{r}
#Show best model
rf_resample %>%
  show_best(n=1)

#Select best model
best_rf <- rf_resample %>%
  select_best(method = "rmse")
```

Random forest model RMSE: 1.20

### Create final fit based on best model

```{r}
rf_final_wf <- 
  rf_wf %>% 
  finalize_workflow(best_rf) #update workflow with best model

rf_final_fit <- 
  rf_final_wf %>%
  fit(train) #fit best model to training data
```

### Evaluate final fit

#### Calculating residuals

```{r}
rf_residuals <- rf_final_fit %>%
  augment(train) %>% #use augment() to make predictions from train data
  select(c(.pred, BodyTemp)) %>%
  mutate(.resid = BodyTemp - .pred) #calculate residuals and make new row.

rf_residuals
```

#### Plot predictions vs. true value

```{r}
rf_pred_plot <- ggplot(rf_residuals, 
                          aes(x = BodyTemp, 
                              y = .pred)) + 
  geom_point() + 
  labs(title = "Predictions vs Actual: Random Forest", 
       x = "Body Temperature Actual", 
       y = "Body Temperature Prediction") +
   stat_poly_line() +
  stat_poly_eq()
rf_pred_plot
```

There is a stronger correlation here than in the LASSO or Tree Model

#### Plot residuals vs. predictions

```{r}
rf_residual_plot <- ggplot(rf_residuals, 
                              aes(y = .resid, 
                                  x = .pred)) + 
  geom_point() + 
  labs(title = "Predictions vs Residuals: Random Forest", 
       x = "Body Temperature Prediction", 
       y = "Residuals") +
   stat_poly_line() +
  stat_poly_eq()
plot(rf_residual_plot) #view plot
```

There is a slight correlation here, which is not ideal.

# Model Selection

| Model         | RMSE | Std_Err |
|---------------|------|---------|
| Null Train    | 1.21 | 0.018   |
| Null Test     | 1.16 | 0.029   |
| Tree          | 1.23 | 0.016   |
| LASSO         | 1.18 | 0.017   |
| Random Forest | 1.20 | 0.017   |

: Table 1: Metrics for Models

Of the three machine learning models we generated, LASSO had the lowest RMSE, but they were all close. The disadvantage with LASSO though is there is a weak correlation between actual value and prediction. However, the advantage of this model is there is no correlation between predictions and residuals.

The Random Forest model also performed decently with the next lowest RMSE. The disadvantage with this model is that there is a slight correlation between predictions and residuals (as prediction value increases, so does the residual), but the advantage is that there is a stronger correlation between actual values and predictions than in the LASSO model.

The Tree model performed the poorest, being the only one that did not out-perform the null model. The plots generated for this model also did not look promising.

I have decided on the Random Forest model as the final model.

# Final Evaluation

Once you picked your final model, you are allowed to once -- **and only once** -- fit it to the test data and check how well it performs on that data. You can do that using the `last_fit()` function applied to the model you end up choosing. For the final model applied to the test set, report performance and the diagnostic plots as above.

```{r}
rf_last_fit <- rf_final_wf %>% 
  last_fit(data_split)

rf_last_fit %>% collect_metrics()
```

**RMSE of 1.10 and standard error of 0.012.**

**This performance is slightly better than the null model.**\

\
